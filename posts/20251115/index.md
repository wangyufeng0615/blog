---
title: "一些对AI未来的预测"
date: 2025-11-15T00:00:00+08:00
draft: false
---

我想记录一下我对未来关于AI的一些猜想和预测。不是为了证明什么，只是想在未来的某个时间，方便回看自己今天的想法，也许其中有些可以进行修订，有些可以进行强化。

距离我第一次读《浪潮之巅》已经过去11年了，真是令人难以置信的时间尺度。那时候我在上高三，我不记得当时我是怎么了解到这本书的，但我大概能推断出来当时我为什么喜欢这本书：我对身处一个封闭空间的耐受度非常低，读这本书让我的心灵可以短暂地飘向远方。何况，对我这种人来说，要是把发呆和打篮球的时间，用于刷那个我本来也做不太出来的解析几何最后一道小题，只会适得其反，让我考的更差。

《浪潮之巅》开篇，吴军老师说的一句话让我终生难忘。大意是说，对于一个年轻人来说，最幸运的事情莫过于赶上一次大潮，一生赶上一次就足够了。

对于那时候的我来说，当然还不知道什么是浪潮，不知道什么是互联网大厂，当然也不知道人工智能、区块链。但这句话似乎是种在了我心里。用那句《我与地坛》读后感的名言来说，这18岁的时候开出的一枪，的确是在我将近30岁的时候，在轻轻的呼啸声中，有了一种正中眉心的感觉。对我来说，这轻轻的呼啸声就是今天的AI浪潮。

以下是观点，涉及到一些术语，如果您不是程序员的话，不必深究。正如我所说，我主要是为了给自己的想法做个记录，方便今后回看。

观点1：智能还远远不够便宜

今天的智能还太贵了。这里说的"智能太贵"放在今天的话，可以理解为大模型输出token的价格。我之前用过Cursor写代码，白天写公司的需求，晚上做点自己的项目，一开始用着挺爽的，后来有一天改了计费模式，我几乎是1天之内用掉了100美元的额度。据我大致的了解，比如claude code这样的工具，包月模式的话做的都是亏本买卖，一方面是花融资的钱，另外一方面是做各种限流。

也许对于一个程序员个体来说，claude code 200美元每月的套餐，或者勉强一点，codex 20美元一个月的套餐也许都够用了，但对于软件工程中所需要的智能来说，绝对远远不够。因为对于一个程序员来说，它对LLM所发出的请求基本都是人肉触发的，LLM运行的时间在一天24小时的占比是相对较少的。但对一个软件项目来说，尤其是一个有点规模的项目来说，它需要的是7x24小时，无数个Agent并行运行的环境。

如果只是一个程序员搭配一个coding agent，比如claude code的话，哪怕让这个人996地工作，也一定是有显著瓶颈的。当然，如果一个团队里的人都用上了coding agent，开发效率会比传统的软件开发流程有数量级级别的提升，但和我所设想的那种场景，也会有数量级级别的差距。

这种场景具体来说，就是每个agent都会有自己的环境，它会自己在环境中和代码进行交互，有些交互是人类所指定的，有些交互甚至不需要人类的指定。比如说，有些agent自己就会尝试重构代码，有些agent 7x24小时不知疲倦地去进行接口测试并补充测试用例，有些agent自己就会根据公司同事的聊天记录去分析潜在的需求，然后自己开发功能，并最终交付给人类工程师进行review。

这些事情超出当今AI的能力了吗，我认为都没有，如果给每个潜在的Agent任务都搭配一个人类工程师，我相信这些事情都是可以被处理的（也许这就是当今的现状）。但我相信随着Token输入输出成本的快速降低（我希望这个趋势至少不能落后于摩尔定律，也就是每18个月成本降一半），人类工程师和Agent的配比会快速下降，比如会演变为1个人类工程师搭配上百个7x24小时并行工作的Agent。

至于为什么今天还没能做到这样，我想除了LLM能力本身的局限外，更大的阻碍还是成本太贵。除了软件工程，我相信人类社会中还有很多很多事情，需要去7x24小时地并行运行多个AI Agent。比如说，我有1TB的照片，我希望让AI去基于我的这些照片，写出我的人生回忆录。理论上现在能不能做到呢，我自己写个调用openrouter的脚本我相信也能做到，但就是太贵了。

比如说，对于机器人来说，输入输出的token流就是7x24小时的，对于一家公司来说，内部所产生的数据（聊天记录，代码，日志，视频，图像）也是接近7x24小时的。我相信对于未来的AI来说，必然会走到这样的局面：所有的历史数据早就已经被完全挖掘、用于训练或推理了，而一切实时产生的新数据流（当然是其中对AI有价值的那部分，但也几乎就是全部），都会小于这个公司、组织或者个人所使用的AI系统的输入带宽。也就是说，一切新的数据都会被使用，也许用于训练，也许用于推理。

总之，这一切都需要智能变得足够便宜。我相信在未来，智能的价格会非常接近于水电，也就是如果一个人想要获取一个"日常生活助手"级别的智能，它的价格不会比每个月付出的水电费多出太多。

观点2：存储还远远不够

就像我刚才提到的，在无数个并行环境中7x24小时地运行Agent，所产生的数据一定是非常宝贵的，毕竟如果到了这种局面的时候，人类历史上所创造的所有数据恐怕都已经被用于训练了，或者说今天就已经接近这一情况了。

所以，在我个人的推断里，未来的大模型还想进步，除了在算法、架构上继续升级外，还需要保存下来非常非常多的数据，而且因为这些数据是要用于训练或者推理，所以不能只存在便宜的HDD里。HBM、DRAM、NAND，应该都会有需求上的暴涨。有人说这些存储需求的暴涨是因为什么sora2、chatgpt pulse功能之类的，我是想说这些功能对存储的需求程度，和我刚才提到的这种场景相比，可以说是微不足道。

作为游戏玩家，做个预测：今天的游戏下载的时候的大小都是固定的，比如70GB，顶多因为存档的增多，再增加个几百MB。以后的游戏所消耗的磁盘空间，会是动态增长的，比如下载下来的时候50GB，玩了几个月后就会变成100GB。以后游戏对冷存储（SSD+HDD）的开销，会有和对内存的开销一样的趋势，也就是会动态增加。这些增加的数据是玩家和游戏世界交互过程中所产生的数据，并进一步用于让游戏带给玩家新的体验。

观点3：非常看好人形机器人

还没有很成熟的观点，只是简单记录一下。毕竟在一切对未来的畅享中，人类还是最终的需求方，是终极的客户。所以，人类的那些最根本的需求，动物性的需求，不会因为AI时代到来了就改变了。AI算法的进步速度，还是比人类基因的演化速度快上太多了。

人类会有对安全感的需求，对有尊严地活着的需求，对陪伴的需求，对动物性本能的需求，等等。如果等人形机器人在智能方面（这个我认为完全没问题），在成本方面（可能会稍微有点漫长），在能源方面（其实这个感觉反而可能是最悲观的）有像ChatGPT 3.5那样的突破的话，人形机器人会深刻重塑人类社会、人类的道德观念、人类对人与人之间关系的理解，这是我的看法。

—

也许人形机器人还是有点遥远了，但足够充分的存储对智能进化速度的加速，我认为在2026年就会初现端倪。足够便宜的智能，也许会更慢一点，但我相信在2030年前一定会实现。现在说起来2030年觉得很遥远，也许是因为AI的世界一切都足够快吧，毕竟，距离ChatGPT 3.5的出现也才马上要满3年，而Deepseek也只是今年春节的事情。我有时候想到，当今绝大多数大模型的训练和推理，还是跑在nvidia的H100上，我就对Blackwell系列大规模使用后的图景充满了期待，那其实也就是2026年。

至少在AI浪潮这件事上，我觉得我虽然算不上运气最好的那拨人，但也不算太差。要是早个5年，我的心思也根本就不在这些事上，如果再晚个5年，我又会比今天更频繁的感慨自己是否已经步入中年，发出更多无意义的感叹。

其实，哪怕AI技术就此永远停滞，再也不进步了，我也已经感到足够幸运了。我这两年大概捣鼓了10多个小代码项目，大概上线了其中的3-4个，如果没有AI，尤其是没有claude code的话，其中99%的创意只能停留在想法上，毕竟，白天都用来上班了。但显然，从今天的趋势来看，就此停滞是不可能的，甚至我在想，如果这是一个加速的环节，我们是否还甚至没切到2档。

我知道这个我们所生活的这个地球上，有很多人类从未使用过，听说过人工智能，也许再过上20年、30年，也是如此。不知道为什么，这些天发呆的时候，我偶尔会想到苏丹内战里那些被杀死的人，也许这些人没人用过ChatGPT。以前经常听一些播客会聊到年轻人空心化的事情，都说要找到工作之外意义的锚，要找到你的内在记分牌，等等。我这里简单记录一个非常简单的想法，也许一年之后就会改变，也许我会彻底忘记，但都没关系：就像传教士在大航海时代做的事情一样，如果说人工智能真的对人类是有益的，不仅仅是对人类整体有益，而是对每个人类个体的福祉都有益，对那些最为具体的人类的需求有益，比如感到更安全，住的更温暖，更少生病，更多满足，那我想要投身于这样的事业，将人工智能的益处去传播到这个地球上每个有人类存在的角落。当然了，也许我现在做的工作就是这样宏大事业的一部分了，不得不说是有这个可能性的。
